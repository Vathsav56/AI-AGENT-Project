{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK4AnWg2/iObu9qQXwLKNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vathsav56/AI-AGENT-Project/blob/main/code/Web_finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVkPbdh79FD6",
        "outputId": "91ae6bae-f368-428c-97f7-dc9b2e76f2dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: googlesearch-python\n",
            "Successfully installed googlesearch-python-1.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install googlesearch-python requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import webbrowser\n",
        "def get_topic_info(topic):\n",
        "\n",
        "    important_points = []\n",
        "    print(f\"Searching the web for: {topic}...\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        for url in search(topic, num_results=5, lang='en'):\n",
        "            print(f\"Checking general info URL: {url}\")\n",
        "            try:\n",
        "                response = requests.get(url, timeout=5)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                paragraphs = soup.find_all('p')\n",
        "                for p in paragraphs[:3]:\n",
        "                    text = p.get_text(strip=True)\n",
        "                    if len(text) > 50:\n",
        "                        important_points.append(text)\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Could not access {url}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing {url}: {e}\")\n",
        "\n",
        "            if len(important_points) > 10:\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during web search for general info: {e}\")\n",
        "\n",
        "    return important_points\n",
        "\n",
        "def find_and_display_image(topic):\n",
        "\n",
        "    print(f\"\\nSearching Google Images for: {topic}...\")\n",
        "    image_search_query = f\"{topic} image\"\n",
        "    direct_image_url = None\n",
        "\n",
        "    try:\n",
        "        for url in search(image_search_query, num_results=5, lang='en', safe='on'):\n",
        "            if \"images.google.com\" in url or \"google.com/imgres\" in url:\n",
        "                print(f\"Checking image search result URL: {url}\")\n",
        "                try:\n",
        "                    response = requests.get(url, timeout=5)\n",
        "                    response.raise_for_status()\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "                    for meta in soup.find_all('meta', property=\"og:image\"):\n",
        "                        direct_image_url = meta.get('content')\n",
        "                        if direct_image_url and (direct_image_url.startswith('http') or direct_image_url.startswith('https')):\n",
        "                            print(f\"Found potential direct image URL from og:image: {direct_image_url}\")\n",
        "                            break\n",
        "\n",
        "                    if not direct_image_url:\n",
        "\n",
        "                        for img_tag in soup.find_all('img', src=True):\n",
        "                            src = img_tag.get('src')\n",
        "                            if src and (src.startswith('http') or src.startswith('https')) and ('q=tbn' not in src): # Filter out thumbnails\n",
        "                                if any(ext in src for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
        "                                    direct_image_url = src\n",
        "                                    print(f\"Found potential direct image URL from img src: {direct_image_url}\")\n",
        "                                    break\n",
        "                    if direct_image_url:\n",
        "                        break\n",
        "\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Could not access {url} for image: {e}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing {url} for image: {e}\")\n",
        "            if direct_image_url:\n",
        "                break\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Google Images search: {e}\")\n",
        "\n",
        "    if direct_image_url:\n",
        "        print(f\"\\nOpening image in your default web browser: {direct_image_url}\")\n",
        "        try:\n",
        "            webbrowser.open_new_tab(direct_image_url)\n",
        "            return direct_image_url\n",
        "        except Exception as e:\n",
        "            print(f\"Could not open browser: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"\\nCould not find a direct image URL to display.\")\n",
        "        return None\n",
        "\n",
        "def create_info_file(topic, points, image_url=None):\n",
        "    \"\"\"\n",
        "    Creates a text file with the extracted important points and an image URL (if found).\n",
        "    \"\"\"\n",
        "    filename = f\"{topic.replace(' ', '_').lower()}_info.txt\"\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- Information about: {topic.upper()} ---\\n\\n\")\n",
        "            f.write(\"Important Points:\\n\")\n",
        "            if points:\n",
        "                for i, point in enumerate(points):\n",
        "                    f.write(f\"{i+1}. {point}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"No significant points found.\\n\\n\")\n",
        "\n",
        "            if image_url:\n",
        "                f.write(\"\\n--- Image Displayed (Opened in Browser) ---\\n\")\n",
        "                f.write(f\"The image was opened in your default web browser. You can also view it at:\\n{image_url}\\n\")\n",
        "            else:\n",
        "                f.write(\"No direct image was found or opened in browser.\\n\")\n",
        "\n",
        "        print(f\"\\nInformation saved to: {filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing to file {filename}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topic_input = input(\"Enter the topic you want to search for: \")\n",
        "\n",
        "\n",
        "    points = get_topic_info(topic_input)\n",
        "\n",
        "\n",
        "    found_image_url = find_and_display_image(topic_input)\n",
        "\n",
        "\n",
        "    create_info_file(topic_input, points, found_image_url)\n",
        "\n",
        "    print(\"\\n--- Process Complete ---\")\n",
        "    print(\"Please review the generated text file for information.\")\n",
        "    if found_image_url:\n",
        "        print(\"An image related to your topic should have opened in your browser.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOhq_trZ950a",
        "outputId": "568e554d-8eef-43bf-f618-fe381e5a6ef4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the topic you want to search for: nvidia stock\n",
            "Searching the web for: nvidia stock...\n",
            "Checking general info URL: https://finance.yahoo.com/news/nvidia-earnings-trump-tariff-updates-and-the-feds-preferred-inflation-gauge-what-to-know-this-week-113221140.html\n",
            "Could not access https://finance.yahoo.com/news/nvidia-earnings-trump-tariff-updates-and-the-feds-preferred-inflation-gauge-what-to-know-this-week-113221140.html: 429 Client Error: Too Many Requests for url: https://finance.yahoo.com/news/nvidia-earnings-trump-tariff-updates-and-the-feds-preferred-inflation-gauge-what-to-know-this-week-113221140.html\n",
            "Checking general info URL: https://www.nvidia.com/en-us/about-nvidia/legal-info/logo-brand-usage/\n",
            "Checking general info URL: https://investor.nvidia.com/stock-info/stock-quote-and-chart/default.aspx\n",
            "Could not access https://investor.nvidia.com/stock-info/stock-quote-and-chart/default.aspx: 403 Client Error: Forbidden for url: https://investor.nvidia.com/stock-info/stock-quote-and-chart/default.aspx\n",
            "Checking general info URL: https://finance.yahoo.com/quote/NVDA/\n",
            "Could not access https://finance.yahoo.com/quote/NVDA/: 429 Client Error: Too Many Requests for url: https://finance.yahoo.com/quote/NVDA/\n",
            "Checking general info URL: https://www.investopedia.com/what-analysts-think-of-nvidia-stock-ahead-of-earnings-11740324\n",
            "\n",
            "Searching Google Images for: nvidia stock...\n",
            "\n",
            "Could not find a direct image URL to display.\n",
            "\n",
            "Information saved to: nvidia_stock_info.txt\n",
            "\n",
            "--- Process Complete ---\n",
            "Please review the generated text file for information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BIOHLadG-Tya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}